# -*- coding: utf-8 -*-
"""StyleGAN of audio_to_visual_prototype_mk1.5_AI integration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16WHm_0SyYpJ797yys2sevc4mnPhFXmEB

###Updated code for StyleGAN2-ADA
"""

# check Python version
!python --version

# install required packages
!pip install torch torchvision librosa matplotlib tensorflow numpy scipy pillow requests tqdm

# Commented out IPython magic to ensure Python compatibility.
# Clone the StyleGAN2-ADA repository
!git clone https://github.com/NVlabs/stylegan2-ada.git
# %cd stylegan2-ada

# install dependencies
!pip install -r requirements.txt

# Download pre-trained StyleGAN2-ADA weights (FFHQ model)
!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl

"""###Audio processing"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

# access file in Google Drive:
from google.colab import files
file_path = '/content/drive/My Drive/sumu - apart [NCS Release].wav'
y, sr = librosa.load(file_path, sr=22050)

# Extract Mel-Spectrogram
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

# Visualize Spectrogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', fmax=8000)
plt.colorbar(format='%+2.0f dB')
plt.title('Mel-Spectrogram')
plt.tight_layout()
plt.show()

"""###VAE for Latent Space Encoding"""

import torch
from torch import nn

# Define VAE
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        return z  # Only returning latent vector

# Instantiate VAE
latent_dim = 512  # StyleGAN2-ADA latent dimension
hidden_dim = 1024
input_dim = mel_spec_db.shape[0] * mel_spec_db.shape[1]  # Flattened spectrogram
vae = VAE(input_dim, hidden_dim, latent_dim)

# Generate Latent Vector
mel_spec_flat = mel_spec_db.flatten().astype(np.float32)
mel_spec_tensor = torch.tensor(mel_spec_flat).unsqueeze(0)  # Add batch dimension
latent = vae(mel_spec_tensor)
latent_vector = latent.detach().numpy()
print("Latent Vector Shape:", latent_vector.shape)

"""###StyleGAN2 Integration

"""

import dnnlib
import legacy
import numpy as np
import PIL.Image

# Load Pre-Trained StyleGAN2 Model
model_path = '/content/drive/My Drive/ffhq.pkl' # file path to ffhq.pkl direct from drive
with dnnlib.util.open_url(model_path) as f:
    G = legacy.load_network_pkl(f)['G_ema']  # Load the generator

# Generate visuals using the latent vector
latent_vector = np.random.randn(1, G.z_dim)  # Random latent vector
img = G.synthesis(torch.from_numpy(latent_vector).to(torch.float32).cuda(), noise_mode='const')
img = (img + 1) * (255 / 2)  # Normalize to [0, 255]
img = np.clip(img.cpu().numpy().transpose(0, 2, 3, 1)[0], 0, 255).astype(np.uint8)

# Visualize the generated image
PIL.Image.fromarray(img).show()

"""###Video Generation"""

import cv2

# Initialize video writer
frame_size = (1024, 1024)  # StyleGAN2-ADA output resolution
out = cv2.VideoWriter('stylegan_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 10, frame_size)

# Generate and save frames
for i in range(100):  # Generate 100 frames
    latent_vector = np.random.randn(1, G.z_dim)  # Random latent vector
    img = G.synthesis(torch.from_numpy(latent_vector).to(torch.float32).cuda(), noise_mode='const')
    img = (img + 1) * (255 / 2)  # Normalize to [0, 255]
    img = np.clip(img.cpu().numpy().transpose(0, 2, 3, 1)[0], 0, 255).astype(np.uint8)

    # Write to video file
    out.write(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))

out.release()
print("Video saved as 'stylegan_video.mp4'")
